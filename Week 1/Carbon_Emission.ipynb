{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102ba60c-8170-48ac-a7a9-68bd1939c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assistant\n",
    "# First, install the missing dependency\n",
    "!pip install xlrd>=2.0.1\n",
    "\n",
    "# After installation, import pandas if not already imported\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# define the file name and the data sheet\n",
    "orig_data_file = r\"climate_change_data.xls\"\n",
    "data_sheet = \"Data\"\n",
    "\n",
    "# read the data from the excel file to a pandas DataFrame\n",
    "data_orig = pd.read_excel(io=orig_data_file, sheet_name=data_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2092746-b94c-4706-b3af-de69c2ab18df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows:\n",
      "13512\n",
      "Current number of rows:\n",
      "10017\n"
     ]
    }
   ],
   "source": [
    "# assign the data to a new DataFrame, which will be modified\n",
    "data_clean = data_orig\n",
    "\n",
    "print(\"Original number of rows:\")\n",
    "print(data_clean.shape[0])\n",
    "\n",
    "# remove rows characterized as \"Text\" in the SCALE column\n",
    "data_clean = data_clean[data_clean['SCALE']!='Text']\n",
    "\n",
    "print(\"Current number of rows:\")\n",
    "print(data_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312f0c1c-fe01-4c92-b820-d135be401a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of columns:\n",
      "28\n",
      "Current number of columns:\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "print(\"Original number of columns:\")\n",
    "print(data_clean.shape[1])\n",
    "\n",
    "data_clean = data_clean.drop(['Country name', 'Series code', 'SCALE', 'Decimals'], axis='columns')\n",
    "\n",
    "print(\"Current number of columns:\")\n",
    "print(data_clean.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9cefa9d-08b1-44de-bb39-6e31ee33e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.iloc[:,2:] = data_clean.iloc[:,2:].replace({'':np.nan, '..':np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5113a90-8212-4809-8115-0364d69856d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the column data types after transformation:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Country code     object\n",
       "Series name      object\n",
       "1990            float64\n",
       "1991            float64\n",
       "1992            float64\n",
       "1993            float64\n",
       "1994            float64\n",
       "1995            float64\n",
       "1996            float64\n",
       "1997            float64\n",
       "1998            float64\n",
       "1999            float64\n",
       "2000            float64\n",
       "2001            float64\n",
       "2002            float64\n",
       "2003            float64\n",
       "2004            float64\n",
       "2005            float64\n",
       "2006            float64\n",
       "2007            float64\n",
       "2008            float64\n",
       "2009            float64\n",
       "2010            float64\n",
       "2011            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean2 = data_clean.applymap(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "# Errors are ignored in order to avoid error messages about the first two columns, which don't need to be transformed\n",
    "# into numeric type anyway\n",
    "\n",
    "print(\"Print the column data types after transformation:\")\n",
    "data_clean2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb7b3e7-b39d-478f-8b48-d158e1ae8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shorter names corresponding to most relevant variables in a dictionary\n",
    "chosen_vars = {'Cereal yield (kg per hectare)': 'cereal_yield',\n",
    "               'Foreign direct investment, net inflows (% of GDP)': 'fdi_perc_gdp',\n",
    "               'Access to electricity (% of total population)': 'elec_access_perc',\n",
    "               'Energy use per units of GDP (kg oil eq./$1,000 of 2005 PPP $)': 'en_per_gdp',\n",
    "               'Energy use per capita (kilograms of oil equivalent)': 'en_per_cap',\n",
    "               'CO2 emissions, total (KtCO2)': 'co2_ttl',\n",
    "               'CO2 emissions per capita (metric tons)': 'co2_per_cap',\n",
    "               'CO2 emissions per units of GDP (kg/$1,000 of 2005 PPP $)': 'co2_per_gdp',\n",
    "               'Other GHG emissions, total (KtCO2e)': 'other_ghg_ttl',\n",
    "               'Methane (CH4) emissions, total (KtCO2e)': 'ch4_ttl',\n",
    "               'Nitrous oxide (N2O) emissions, total (KtCO2e)': 'n2o_ttl',\n",
    "               'Droughts, floods, extreme temps (% pop. avg. 1990-2009)': 'nat_emerg',\n",
    "               'Population in urban agglomerations >1million (%)': 'pop_urb_aggl_perc',\n",
    "               'Nationally terrestrial protected areas (% of total land area)': 'prot_area_perc',\n",
    "               'GDP ($)': 'gdp',\n",
    "               'GNI per capita (Atlas $)': 'gni_per_cap',\n",
    "               'Under-five mortality rate (per 1,000)': 'under_5_mort_rate',\n",
    "               'Population growth (annual %)': 'pop_growth_perc',\n",
    "               'Population': 'pop',\n",
    "               'Urban population growth (annual %)': 'urb_pop_growth_perc',\n",
    "               'Urban population': 'urb_pop'\n",
    "                }\n",
    "\n",
    "# rename all variables in the column \"Series name\" with comprehensible shorter versions\n",
    "data_clean2['Series name'] = data_clean2['Series name'].replace(to_replace=chosen_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c4b96c-c280-47a5-b9a8-7f1ca22d03db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the short feature names into a list of strings\n",
    "chosen_cols = list(chosen_vars.values())\n",
    "\n",
    "# define an empty list, where sub-dataframes for each feature will be saved\n",
    "frame_list = []\n",
    "\n",
    "# iterate over all chosen features\n",
    "for variable in chosen_cols:\n",
    "\n",
    "    # pick only rows corresponding to the current feature\n",
    "    frame = data_clean2[data_clean2['Series name'] == variable]\n",
    "\n",
    "    # melt all the values for all years into one column and rename the columns correspondingly\n",
    "    frame = frame.melt(id_vars=['Country code', 'Series name']).rename(columns={'Country code': 'country', 'variable': 'year', 'value': variable}).drop(['Series name'], axis='columns')\n",
    "\n",
    "    # add the melted dataframe for the current feature into the list\n",
    "    frame_list.append(frame)\n",
    "\n",
    "\n",
    "# merge all sub-frames into a single dataframe, making an outer binding on the key columns 'country','year'\n",
    "from functools import reduce\n",
    "all_vars = reduce(lambda left, right: pd.merge(left, right, on=['country','year'], how='outer'), frame_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f32484c6-6d1e-41eb-a191-2a8dec85bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the amount of missing values in each column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "country                   0\n",
       "year                      0\n",
       "cereal_yield           1377\n",
       "fdi_perc_gdp           1111\n",
       "elec_access_perc       5027\n",
       "en_per_gdp             2082\n",
       "en_per_cap             1956\n",
       "co2_ttl                1143\n",
       "co2_per_cap            1146\n",
       "co2_per_gdp            1557\n",
       "other_ghg_ttl          4542\n",
       "ch4_ttl                4526\n",
       "n2o_ttl                4526\n",
       "nat_emerg              4958\n",
       "pop_urb_aggl_perc      2582\n",
       "prot_area_perc          726\n",
       "gdp                     779\n",
       "gni_per_cap            1013\n",
       "under_5_mort_rate       716\n",
       "pop_growth_perc         278\n",
       "pop                     252\n",
       "urb_pop_growth_perc     490\n",
       "urb_pop                 467\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"check the amount of missing values in each column\")\n",
    "all_vars.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9adbb8e-a031-4d03-92aa-c2e8125e9dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing values by year:\n",
      "2005 : 1189\n",
      "2000 : 1273\n",
      "1995 : 1317\n",
      "1990 : 1427\n",
      "2007 : 1631\n",
      "2006 : 1633\n",
      "2004 : 1646\n",
      "2008 : 1708\n",
      "2003 : 1714\n",
      "2002 : 1715\n",
      "2001 : 1718\n",
      "1999 : 1729\n",
      "1998 : 1739\n",
      "1997 : 1746\n",
      "1996 : 1756\n",
      "1994 : 1781\n",
      "1993 : 1792\n",
      "1992 : 1810\n",
      "1991 : 1921\n",
      "2009 : 2078\n",
      "2010 : 3038\n",
      "2011 : 4893\n"
     ]
    }
   ],
   "source": [
    "all_vars_clean = all_vars\n",
    "\n",
    "#define an array with the unique year values\n",
    "years_count_missing = dict.fromkeys(all_vars_clean['year'].unique(), 0)\n",
    "for ind, row in all_vars_clean.iterrows():\n",
    "    years_count_missing[row['year']] += row.isnull().sum()\n",
    "\n",
    "# sort the years by missing values\n",
    "years_missing_sorted = dict(sorted(years_count_missing.items(), key=lambda item: item[1]))\n",
    "\n",
    "# print the missing values for each year\n",
    "print(\"missing values by year:\")\n",
    "for key, val in years_missing_sorted.items():\n",
    "    print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09407e9c-0a78-4758-822a-7c29a8ba4bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of missing values in the whole dataset before filtering the years:\n",
      "41254\n",
      "number of rows before filtering the years:\n",
      "5126\n",
      "number of missing values in the whole dataset after filtering the years:\n",
      "29818\n",
      "number of rows after filtering the years:\n",
      "4194\n"
     ]
    }
   ],
   "source": [
    "print(\"number of missing values in the whole dataset before filtering the years:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows before filtering the years:\")\n",
    "print(all_vars_clean.shape[0])\n",
    "\n",
    "# filter only rows for years between 1991 and 2008 (having less missing values)\n",
    "all_vars_clean = all_vars_clean[(all_vars_clean['year'] >= 1991) & (all_vars_clean['year'] <= 2008)]\n",
    "\n",
    "print(\"number of missing values in the whole dataset after filtering the years:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows after filtering the years:\")\n",
    "print(all_vars_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8338987-2c73-45b1-ba6c-9895c868a28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing values by country:\n",
      "AGO : 81\n",
      "ARG : 81\n",
      "AUS : 81\n",
      "AUT : 81\n",
      "BGD : 81\n",
      "BGR : 81\n",
      "BOL : 81\n",
      "BRA : 81\n",
      "CAN : 81\n",
      "CHE : 81\n",
      "CHL : 81\n",
      "CHN : 81\n",
      "CIV : 81\n",
      "CMR : 81\n",
      "COG : 81\n",
      "COL : 81\n",
      "CRI : 81\n",
      "DEU : 81\n",
      "DNK : 81\n",
      "DOM : 81\n",
      "ECU : 81\n",
      "EGY : 81\n",
      "EMU : 81\n",
      "ESP : 81\n",
      "FIN : 81\n",
      "FRA : 81\n",
      "GBR : 81\n",
      "GHA : 81\n",
      "GTM : 81\n",
      "HND : 81\n",
      "HUN : 81\n",
      "IDN : 81\n",
      "IND : 81\n",
      "IRL : 81\n",
      "ISR : 81\n",
      "ITA : 81\n",
      "JOR : 81\n",
      "JPN : 81\n",
      "KEN : 81\n",
      "KOR : 81\n",
      "LAC : 81\n",
      "LMC : 81\n",
      "LMY : 81\n",
      "MAR : 81\n",
      "MEX : 81\n",
      "MIC : 81\n",
      "MNA : 81\n",
      "MOZ : 81\n",
      "MYS : 81\n",
      "NGA : 81\n",
      "NLD : 81\n",
      "NZL : 81\n",
      "PAK : 81\n",
      "PAN : 81\n",
      "PER : 81\n",
      "PHL : 81\n",
      "PRT : 81\n",
      "PRY : 81\n",
      "ROM : 81\n",
      "SAS : 81\n",
      "SAU : 81\n",
      "SDN : 81\n",
      "SEN : 81\n",
      "SLV : 81\n",
      "SWE : 81\n",
      "SYR : 81\n",
      "TGO : 81\n",
      "THA : 81\n",
      "TUR : 81\n",
      "TZA : 81\n",
      "UMC : 81\n",
      "URY : 81\n",
      "USA : 81\n",
      "VEN : 81\n",
      "VNM : 81\n",
      "ZAF : 81\n",
      "ZMB : 81\n",
      "GRC : 82\n",
      "POL : 82\n",
      "YEM : 82\n",
      "ZAR : 82\n",
      "DZA : 84\n",
      "ETH : 84\n",
      "LIC : 84\n",
      "SSA : 84\n",
      "WLD : 84\n",
      "ARE : 85\n",
      "ECA : 85\n",
      "RUS : 86\n",
      "UKR : 86\n",
      "ARM : 87\n",
      "BLR : 87\n",
      "UZB : 87\n",
      "KAZ : 88\n",
      "CZE : 89\n",
      "IRN : 89\n",
      "BEL : 90\n",
      "AZE : 91\n",
      "GEO : 92\n",
      "LBN : 92\n",
      "HTI : 94\n",
      "NIC : 96\n",
      "BEN : 99\n",
      "BWA : 99\n",
      "CYP : 99\n",
      "GAB : 99\n",
      "HIC : 99\n",
      "JAM : 99\n",
      "KHM : 99\n",
      "LKA : 99\n",
      "MLT : 99\n",
      "MNG : 99\n",
      "NOR : 99\n",
      "OMN : 99\n",
      "SGP : 99\n",
      "TTO : 99\n",
      "TUN : 99\n",
      "ALB : 100\n",
      "EAP : 102\n",
      "NPL : 103\n",
      "EST : 104\n",
      "LVA : 104\n",
      "NAM : 104\n",
      "HRV : 105\n",
      "MDA : 105\n",
      "SVN : 105\n",
      "TJK : 105\n",
      "KGZ : 106\n",
      "LTU : 106\n",
      "MKD : 107\n",
      "SVK : 107\n",
      "TKM : 107\n",
      "LBY : 108\n",
      "LUX : 108\n",
      "BRN : 109\n",
      "KWT : 113\n",
      "BHR : 117\n",
      "CUB : 117\n",
      "ISL : 117\n",
      "ZWE : 117\n",
      "ERI : 121\n",
      "IRQ : 122\n",
      "BIH : 123\n",
      "HKG : 124\n",
      "BFA : 126\n",
      "GIN : 126\n",
      "MDG : 126\n",
      "MLI : 126\n",
      "NER : 126\n",
      "UGA : 126\n",
      "QAT : 135\n",
      "ATG : 136\n",
      "BHS : 136\n",
      "BLZ : 136\n",
      "BRB : 136\n",
      "COM : 136\n",
      "CPV : 136\n",
      "DMA : 136\n",
      "FJI : 136\n",
      "GNB : 136\n",
      "GRD : 136\n",
      "GUY : 136\n",
      "MUS : 136\n",
      "SWZ : 136\n",
      "VCT : 136\n",
      "VUT : 136\n",
      "DJI : 137\n",
      "SLB : 137\n",
      "SUR : 137\n",
      "GMB : 141\n",
      "BDI : 144\n",
      "CAF : 144\n",
      "LAO : 144\n",
      "MRT : 144\n",
      "MWI : 144\n",
      "PNG : 144\n",
      "RWA : 144\n",
      "SLE : 144\n",
      "TCD : 144\n",
      "BTN : 148\n",
      "LBR : 149\n",
      "SID : 152\n",
      "GNQ : 154\n",
      "KNA : 154\n",
      "LCA : 154\n",
      "SYC : 154\n",
      "TON : 154\n",
      "WSM : 154\n",
      "KIR : 156\n",
      "SRB : 158\n",
      "MDV : 164\n",
      "PLW : 166\n",
      "AFG : 170\n",
      "MMR : 171\n",
      "PRK : 171\n",
      "FSM : 184\n",
      "MHL : 184\n",
      "LSO : 190\n",
      "MAC : 198\n",
      "STP : 198\n",
      "TMP : 202\n",
      "NCL : 204\n",
      "ADO : 206\n",
      "SOM : 206\n",
      "WBG : 207\n",
      "GRL : 216\n",
      "ABW : 226\n",
      "BMU : 226\n",
      "PRI : 230\n",
      "MNE : 231\n",
      "PYF : 232\n",
      "LIE : 234\n",
      "MCO : 234\n",
      "CYM : 250\n",
      "FRO : 259\n",
      "GIB : 261\n",
      "COK : 270\n",
      "GUM : 270\n",
      "NIU : 270\n",
      "SMR : 270\n",
      "TUV : 272\n",
      "IMY : 282\n",
      "VIR : 285\n",
      "ASM : 288\n",
      "CHI : 288\n",
      "MNP : 288\n",
      "NRU : 288\n",
      "TCA : 296\n",
      "MYT : 324\n",
      "KSV : 325\n",
      "MAF : 342\n",
      "CUW : 357\n",
      "SXM : 357\n"
     ]
    }
   ],
   "source": [
    "# check the amount of missing values by country\n",
    "\n",
    "# define an array with the unique country values\n",
    "countries_count_missing = dict.fromkeys(all_vars_clean['country'].unique(), 0)\n",
    "\n",
    "# iterate through all rows and count the amount of NaN values for each country\n",
    "for ind, row in all_vars_clean.iterrows():\n",
    "    countries_count_missing[row['country']] += row.isnull().sum()\n",
    "\n",
    "# sort the countries by missing values\n",
    "countries_missing_sorted = dict(sorted(countries_count_missing.items(), key=lambda item: item[1]))\n",
    "\n",
    "# print the missing values for each country\n",
    "print(\"missing values by country:\")\n",
    "for key, val in countries_missing_sorted.items():\n",
    "    print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91dd86af-81ba-4b16-ab92-5d4b712eaf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of missing values in the whole dataset before filtering the countries:\n",
      "29818\n",
      "number of rows before filtering the countries:\n",
      "4194\n",
      "number of missing values in the whole dataset after filtering the countries:\n",
      "7854\n",
      "number of rows after filtering the countries:\n",
      "1728\n"
     ]
    }
   ],
   "source": [
    "print(\"number of missing values in the whole dataset before filtering the countries:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows before filtering the countries:\")\n",
    "print(all_vars_clean.shape[0])\n",
    "\n",
    "\n",
    "# filter only rows for countries with less than 90 missing values\n",
    "countries_filter = []\n",
    "for key, val in countries_missing_sorted.items():\n",
    "    if val<90:\n",
    "        countries_filter.append(key)\n",
    "\n",
    "all_vars_clean = all_vars_clean[all_vars_clean['country'].isin(countries_filter)]\n",
    "\n",
    "print(\"number of missing values in the whole dataset after filtering the countries:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows after filtering the countries:\")\n",
    "print(all_vars_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57ae5abc-71c6-4c8d-b651-6bba3847c245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country                   0\n",
       "year                      0\n",
       "cereal_yield             10\n",
       "fdi_perc_gdp             17\n",
       "elec_access_perc       1728\n",
       "en_per_gdp                0\n",
       "en_per_cap                0\n",
       "co2_ttl                   9\n",
       "co2_per_cap               9\n",
       "co2_per_gdp               9\n",
       "other_ghg_ttl          1446\n",
       "ch4_ttl                1440\n",
       "n2o_ttl                1440\n",
       "nat_emerg              1728\n",
       "pop_urb_aggl_perc         0\n",
       "prot_area_perc            0\n",
       "gdp                       2\n",
       "gni_per_cap              16\n",
       "under_5_mort_rate         0\n",
       "pop_growth_perc           0\n",
       "pop                       0\n",
       "urb_pop_growth_perc       0\n",
       "urb_pop                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vars_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5498dc29-ac48-4a15-9b34-d4bf46689d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values per column:\n",
      "country                 0\n",
      "year                    0\n",
      "cereal_yield           10\n",
      "fdi_perc_gdp           17\n",
      "en_per_gdp              0\n",
      "en_per_cap              0\n",
      "co2_ttl                 9\n",
      "co2_per_cap             9\n",
      "co2_per_gdp             9\n",
      "pop_urb_aggl_perc       0\n",
      "prot_area_perc          0\n",
      "gdp                     2\n",
      "gni_per_cap            16\n",
      "under_5_mort_rate       0\n",
      "pop_growth_perc         0\n",
      "pop                     0\n",
      "urb_pop_growth_perc     0\n",
      "urb_pop                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# remove features with more than 20 missing values\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "# create a boolean mapping of features with more than 20 missing values\n",
    "vars_bad = all_vars_clean.isnull().sum()>20\n",
    "\n",
    "# remove the columns corresponding to the mapping of the features with many missing values\n",
    "all_vars_clean2 = all_vars_clean.drop(compress(data = all_vars_clean.columns, selectors = vars_bad), axis='columns')\n",
    "\n",
    "print(\"Remaining missing values per column:\")\n",
    "print(all_vars_clean2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c46fc68e-eeea-4b90-91c0-0079667a2a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values per column:\n",
      "country                0\n",
      "year                   0\n",
      "cereal_yield           0\n",
      "fdi_perc_gdp           0\n",
      "en_per_gdp             0\n",
      "en_per_cap             0\n",
      "co2_ttl                0\n",
      "co2_per_cap            0\n",
      "co2_per_gdp            0\n",
      "pop_urb_aggl_perc      0\n",
      "prot_area_perc         0\n",
      "gdp                    0\n",
      "gni_per_cap            0\n",
      "under_5_mort_rate      0\n",
      "pop_growth_perc        0\n",
      "pop                    0\n",
      "urb_pop_growth_perc    0\n",
      "urb_pop                0\n",
      "dtype: int64\n",
      "Final shape of the cleaned dataset:\n",
      "(1700, 18)\n"
     ]
    }
   ],
   "source": [
    "# delete rows with any number of missing values\n",
    "all_vars_clean3 = all_vars_clean2.dropna(axis='rows', how='any')\n",
    "\n",
    "print(\"Remaining missing values per column:\")\n",
    "print(all_vars_clean3.isnull().sum())\n",
    "\n",
    "print(\"Final shape of the cleaned dataset:\")\n",
    "print(all_vars_clean3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df9d608e-5a95-4ecb-977e-ec1cb679532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the clean dataframe to a csv file\n",
    "all_vars_clean3.to_csv('data_cleaned.csv', index=False)\n",
    "\n",
    "# export the clean dataframe to an excel file\n",
    "all_vars_clean3.to_excel('data_cleaned.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
